Scrapy在Python爬虫开发中的应用
Scrapy是一个基于Python语言为了爬取网站数据，提取结构性数据而编写的开源爬虫应用框架。可以运用在包括数据挖掘，信息处理或者存储历史数据等一系列的程序中。本文使用Scrapy框架可以方便地自定义爬虫的爬取规则，同时还有很多稳定的开源库帮助我们进行前置后续处理。实验证明，使用Scrapy开发的效果很好。

3.1 Scrapy 分析与使用

3.1.1 Scrapy 分析

Scrapy是利用Python语言编写的网络爬虫框架。Python是一种具有高度集成性、拥有丰富开源库的计算机程序设计语言，受到诸多程序开发者的青睐。Scrapy最初的设计目的是页面抓取，或者说是网络抓取，当然也可以是用开获取各种API返回的数据。Scrapy作为爬虫爬架，使得爬虫的设计和工作变得快速简单，同时具有高度的扩展性和鲁棒性。所以Scrapy在爬虫设计中使用广泛，也符合本文所设计的爬虫应具有的特性，因此我们使用Scrapy来完成研究。
下图概括了Scrapy的整体架构，Scrapy主要包括了以下组件：
（1）Scrapy Engine:负责处理整个项目的数据流，触发事务
（2）Scheduler ：接收引擎发送过来的请求，压入处理队列，并在引擎再次请求时返回。可以类比为以URL为元素的优先队列，决定了爬虫下一个将要抓取的目标或者网页是什么，同时该部分还要负责URL去重。
（3）Downloader：当爬虫爬取某个网页时，该部分将此网页内容下载至本机，并返回给爬虫。该部分以异步方式工作，所以在多线程模型中将发挥很大的作用。
（4）Spiders ：整个爬虫项目中的核心部分，用于从特定的网页结构中提取目标信息，即所谓的尸体（Item）。在设计中，用户可以规定爬虫提取网页中的URL并返回，使其自动运行爬取。
（5）Pipeline ：Pipeline是负责处理爬虫从网页中抽取的实体，比如结构化数据、转存数据库等。当目标被爬虫获取并解析后，就会被发送到Pipeline，经过其中自定义的方法进行依次处理。
（6）Downloader MIddlewares ：位于Engine和downloader之间，负责处理这二者之间的request以及response
（7） spider MIddlewares 介于Engine和spider之间，处理爬虫的响应输出和请求输入。
（8）scheduler middlewares：位于Engine和scheduler之间，负责从Engine发送到网站或者服务器的请求和响应。
图片图片图片

SCrapy的运行流程基本如下：
1首先，Engine从Scheduler中调取一个URL作为爬取过程的初始目标
2Engine将URL封装为Request传送给Downloader，downloader将资源下载到本机，封装为Response
3Spider接收Response并调用回调函数
4如果从该REsponse中接收到实体，则交给pipeline做后续处理
5如果解析的结果为URL，则将URL发送给Scheduler等待被调用抓取。

scrapy的安装不再赘述。

scrapy的初步使用
创建项目。
scrapy项目的创建或者使用方法非常便利，可以通过命令行的方式（如创建命令：scrapy startproject testspider），也可以使用核心API通过编程的方式。为了获得更高的定制性和灵活性，我们主要使用后者的方式。在这里我们使用了流行的Python语言集成编译环境Pycharm来创建我们的项目，将会得到如下的项目文件夹，如下图所示：

图片图片

大致介绍文件的作用：
1）scrapy.cfg ：该项目的配置文件
2）内层Sina_Spider文件夹：该项目的Python模块，其中包含了项目的主要代码。将在下一章中详细介绍。
3）Begin.py ：为该项目的编译入口，通过运行该文件，将启动爬虫开始工作，即为上文所说核心API编程方式启动。

运行项目 
可以通过控制台使用命令行方式来运行，首先使用“cd”命令进入该项目所在路径，然后调用python解释器运行Begin.py文件，即输入命令
python Begin.py
或者编辑Pycharm的Compiler Configuration，然后点击“run”按钮进行解释运行。
图片图片

Scrapy的其他重点特性：
异步处理：
请求(request)是 被异步调度和处理的 。 这意味着，Scrapy并不需要等待一个请求(request)完成及处理，在此同时， 也发送其他请求或者做些其他事情。 这也意味着，当有些请求失败或者处理过程中出现错误时，其他的请求也能继续处理。
多重控制：
在可以以非常快的速度进行爬取时(以容忍错误的方式同时发送多个request), Scrapy也通过一些设置来允许您控制其爬取的方式。 例如，可以为两个request之间设置下载延迟， 限制单域名(domain)
或单个IP的并发请求量，甚至可以使用自动限制插件来自动处理这些问题。

3.2 数据的存取：
使用Scrapy爬虫得到的数据是返回的结果，如何处理这些数据需要认真的考虑。使用数据库而不是文件来存储，可以使用我们可以进行进一步的处理和分析。所以在这里，我们要要探讨如何使用数据库来存储爬虫返回结果。
3.2.1
返回数据的格式



 3.2.2 MongoDB的特点及使用
 3.2.2.2NoSQL 简介
什么是NoSQL?
NoSQL，指的是非关系型的数据库。NoSQL有时也称作Not Only SQL的缩写，是对不同于传统的关系型数据库的数据库管理系统的统称。
NoSQL用于超大规模数据的存储。（例如谷歌或Facebook每天为他们的用户收集万亿比特的数据）。这些类型的数据存储不需要固定的模式，无需多余操作就可以横向扩展。
为什么使用NoSQL ?
今天我们可以通过第三方平台（如：Google,Facebook等）可以很容易的访问和抓取数据。用户的个人信息，社交网络，地理位置，用户生成的数据和用户操作日志已经成倍的增加。我们如果要对这些用户数据进行挖掘，那SQL数据库已经不适合这些应用了, NoSQL数据库的发展也却能很好的处理这些大的数据。
什么是MongoDB ?
MongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。
在高负载的情况下，添加更多的节点，可以保证服务器性能。
MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。
MongoDB 将数据存储为一个文档，数据结构由键值(key=>value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。
mongodb主要特点
MongoDB的提供了一个面向文档存储，操作起来比较简单和容易。
你可以在MongoDB记录中设置任何属性的索引 (如：FirstName="Sameer",Address="8 Gandhi Road")来实现更快的排序。
你可以通过本地或者网络创建数据镜像，这使得MongoDB有更强的扩展性。
如果负载的增加（需要更多的存储空间和更强的处理能力） ，它可以分布在计算机网络中的其他节点上这就是所谓的分片。
Mongo支持丰富的查询表达式。查询指令使用JSON形式的标记，可轻易查询文档中内嵌的对象及数组。等等



