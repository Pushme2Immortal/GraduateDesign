国内外爬虫技术发展现状

爬虫技术概述及发展历史：
网络爬虫是一个功能很强的自动提取网页的程序, 它为搜索引擎从万维网上下载网页，是搜索引擎的重要组成。 
从上世纪90年代起，就有很多计算机工作者开始从事网络爬虫的开发。目前爬虫技术已经逐渐成熟，很多商业搜索引擎的核心模块就是网络爬虫。网络上比较著名的开源爬虫包括Nutch，Larbin，Heritrix。网络爬虫最重要的是网页搜索策略（广度优先和最佳度优先）和网页分析策略（基于网络拓扑的分析算法和基于网页内容的网页分析算法）。

爬虫技术现状和爬虫设计者面临的问题:
爬虫技术发展到今天，已经有无数的开源库以及开源框架可供我们快速开发，如Scrapy就是其中一种，本文也将就基于Scrapy来实现一个爬虫。我们不再需要从零开始构建一个完整的搜索器，只需要根据所爬取的内容定制爬取规则，就可以很方便的开发。而在诸多爬虫设计者中，公认的难题也早已不是开发周期的问题。
互联网资源数量无法量化，这意味再优秀的网络爬虫也无法爬取当前互联网中所有的资源，因此我们需要不断改进爬虫的性能，优化它的爬取方式。互联网资源瞬息万变，这也意味着网络爬虫下载的网页在使用前就已经被修改甚至是删除了。这对爬虫设计者是不可避免以及必须解决的问题。

再者，服务器端软件所生成的统一资源地址数量庞大，以至于网络爬虫难以避免的采集到重复内容。根据超文本协议“显示请求”（HTTP GET）的参数的无尽组合所返回的页面中，只有很少一部分确实传回唯一的内容。例如：一个照片陈列室网站，可能通过几个参数，让用户选择相关照片：其一是通过四种方法对照片排序，其二是关于照片分辨率的的三种选择，其三是两种文件格式，另加一个用户可否提供内容的选择，这样对于同样的结果集可能会有48种不同的统一资源地址与其关联。这种数学组合给网络爬虫制造了麻烦，因为它们必须越过这些无关脚本变化的组合，寻找到不重复的内容。本研究也将就以上问题，探索解决的方法。