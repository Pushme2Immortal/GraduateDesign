1　网络爬虫的工作原理

网络爬虫通过请求站点上的HTML文档访问某一站点。它遍历Web 空间，不断从一个站点移动到另一个站点，自动建立索引并加入到网页数据库中。网络爬虫进入某个超级文本时, 它利用HTML语言的标记结构来搜索信息及获取指向其他超级文本的URL 地址，可以完全不依赖用户干预实现网络上的自动“爬行”和搜索。网络爬虫在搜索时往往采用一定的搜索策略。
2. 工作流程。
爬虫工作抓取网页的过程其实和读者平时使用IE浏览器浏览网页的道理是一样的。
比如说你在浏览器的地址栏中输入    www.baidu.com    这个地址。
打开网页的过程其实就是浏览器作为一个浏览的“客户端”，向服务器端发送了 一次请求，把服务器端的文件“抓”到本地，再进行解释、展现。
下图所示是一个通用的爬虫框架流程。首先从互联网页面中精心选择一部分网页，以这些网页的链接地址作为种子URL，将这些种子URL放入待抓取URL队列中，爬虫从待抓取URL队列依次读取，并将URL通过DNS解析，把链接地址转换为网站服务器对应的IP地址。

然后将其和网页相对路径名称交给网页下载器，网页下载器负责页面内容的下载。对于下载到本地的网页，一方面将其存储到页面库中，等待建立索引等后续处理;另一方面将下载网页的URL放入已抓取URL队列中，这个队列记载了爬虫系统已经下载过的网页URL，以避免网页的重复抓取。对于刚下载的网页，从中抽取出所包含的所有链接信息，并在已抓取URL队列中检查，如果发现链接还没有被抓取过，则将这个URL放入待抓取URL队列末尾，在之后的抓取调度中会下载这个URL对应的网页。如此这般，形成循环，直到待抓取URL队列为审，这代表着爬虫系统已将能够抓取的网页尽数抓完，此时完成了一轮完整的抓取过程。

对于爬虫来说，往往还需要进行网页去重及网页反作弊。
上述是一个通用爬虫的整体流程，如果从更加宏观的角度考虑，处于动态抓取过程中的爬虫和互联网所有网页之间的关系，可以大致像如图2-2所身那样，将互联网页面划分为5个部分：

1.已下载网页集合：爬虫已经从互联网下载到本地进行索引的网页集合。

2.已过期网页集合：由于网页数最巨大，爬虫完整抓取一轮需要较长时间，在抓取过程中，很多已经下载的网页可能过期。之所以如此，是因为互联网网页处于不断的动态变化过程中，所以易产生本地网页内容和真实互联网网页不一致的情况。

3.待下载网页集合：即处于上图中待抓取URL队列中的网页，这些网页即将被爬虫下载。

4.可知网页集合：这些网页还没有被爬虫下载，也没有出现在待抓取URL队列中，不过通过已经抓取的网页或者在待抓取URL队列中的网页，总足能够通过链接关系发现它们，稍晚时候会被爬虫抓取并索引。

5.不可知网页集合：有些网页对于爬虫来说是无法抓取到的，这部分网页构成了不可知网页集合。事实上，这部分网页所占的比例很高。
