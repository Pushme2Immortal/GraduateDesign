Robots协议也称为爬虫协议、爬虫规则、机器人协议，是互联网行业中约定俗称的一种准入规范。Robot协议的目的在于保护网站内容，包括用户信息、网站数据等。“规则”中将搜索引擎抓取网站内容的范围做了约定,包括网站是否希望被搜索引擎抓取，哪些内容不允许被抓取,而网络爬虫可以据此自动抓取或者不抓取该网页内容。
Robots协议的详解[2]
　　Robots协议是Web提供商和搜索引擎或爬虫交互的一种方式，Robots.txt是存放在站点服务器根目录下的一个纯文本文件。该文件规定了搜索引擎爬虫只抓取指定的内容，或者是禁止搜索引擎爬虫抓取网站的部分或全部内容。当一个搜索引擎爬虫访问一个站点时，它会首先检查该站点根目录下是否存在robots.txt，如果存在，搜索引擎爬虫就会按照该文件中的内容来确定访问的范围；如果该文件不存在，那么搜索引擎爬虫就沿着链接抓取。

具体使用格式如下:

　　(1)User.agent:用于描述访问该站点的客户端的名字标识。被记录在Robot.txt中的User-agent记录将不被允许索引或爬取此站点。通常浏览器的User-agent属性都是被运行的。在Robots.txt文件中，“User-agent:*这样的记录只能有一条。

　　(2)Disallow:用于描述不希望被访问到的一个URL。这个URL标识了某个站点资源的路径，任何被Disallow标记的URL将拒绝搜索引擎的索引或者来自爬虫的访问。

　　大部分商业搜索引擎或者爬虫要会遵守Robots协议并执行Web站点的要求。所以在成熟的搜索模块或者爬虫中，需要设计一个处理站点Robot协议的模块，以增强自身的健壮性。

　　但是Robot协议并不是一个强制性的严格的法律协定，如果搜索引擎爬虫的设计者不遵循这个协议，其设计的代码也将能够
自由爬取站点中的资源。同时，站点管理员也有其他方式来限制网络爬虫在站点的运行。所以，网站与爬虫的关系非常复杂，当我们在设计的过程中，必须要考虑这些限制，来增强爬虫的可行性。


本文实现的爬虫将以"www.sina.com"即新浪微博为爬取目标，为了不受Robot协议的影响，我们将从User-agent入手，尝试解决方法。具体方案将在下章继续陈述。
