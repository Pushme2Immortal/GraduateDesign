<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN""http://www.w3.org/TR/html4/loose.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<title>PaperPass 最权威论文抄袭检测系统</title>
<style type="text/css">
<!--
user_icon {
color: #FFFFFF;
}
html
{
overflow-x:hidden;
overflow-y:auto;
}
body,td,th {
font-family: "微软雅黑";
font-size: 12px;
}
h1,h2,h3,h4,h5,h6 {
font-family: "宋体";
}
p{
margin-bottom:10px;
}
demo_padding {
line-height: 30px;
}
.zhengwen {
padding-right: 15px;
padding-left: 5px;
padding-bottom:100px;
font-size: 13px;
line-height: 20px;
color: #666666;
}
.zhengwencenter {
padding-right: 15px;
padding-left: 0px;
margin-bottom:10px;
font-size: 13px;
line-height: 20px;
color: #666666;
text-align:center
}
.neikuang {
background-color: #EBEBEB;
border: 1px solid #999999;
padding-right: 10px;
padding-left: 10px;
margin-top:10px;
margin-left:25px;
width:300px;
}
.shubu{
height: 20px;
width: 20px;
margin-left:25px;
background-color: #FFFFFF;
border: 1px solid #999999;
text-align: center;
vertical-align: middle;
display: block;
color: #666666;
}
a.red:link {color:#FF0000}
a.red:visited {color:#FF0000}
a.red:hover {color:#000000}
a.red:active {color:#000000}

a.orange:link {color:#FF6600}
a.orange:visited {color:#FF6600}
a.orange:hover {color:#000000}
a.orange:active {color:#000000}

a.dark:link {color:#666666}
a.dark:visited {color:#666666}
a.dark:hover {color:#000000}
a.dark:active {color:#000000}

a.pagelink:hover {color:#000000}
a.pagelink:active {color:#000000}

.green{color:#008000}
.gray{color:#666666}
.red{color:#FF0000}
.orange{color:#FF6600}
a{TEXT-DECORATION:none}

-->
</style>
</head>
<body>


<div class="zhengwen">
<div>
<span style="margin-left:25px"></span>
[
<a class="pagelink" href="paper_1.htm">首页</a>
<a class="pagelink" href="paper_3.htm">上一页</a>
<a class="pagelink" href="paper_5.htm">下一页</a>
<a class="pagelink" href="paper_6.htm">尾页</a>
页码：4/6页
]
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">134</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>3.2.1 NoSQL数据库简介</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">135</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>1. 什么是NoSQL</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">136</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>NoSQL，指的是非关系型的数据库。</span><a href='../sentence_detail/304.htm' target='right' class='red' >NoSQL有时也称作Not Only SQL的缩写，是对不同于传统的关系型数据库的数据库管理系统的统称。</a>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">137</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>NoSQL用于超大规模数据的存储。</span><a href='../sentence_detail/306.htm' target='right' class='red' >（例如谷歌或Facebook每天为他们的用户收集万亿比特的数据）。</a><a href='../sentence_detail/307.htm' target='right' class='red' >这些类型的数据存储不需要固定的模式，无需多余操作就可以横向扩展。</a>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">138</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>2.为什么使用NoSQL</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">139</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>今天我们可以通过第三方平台（如：</span><span class='green'>Google，Facebook等）可以很容易的访问和抓取数据。</span><a href='../sentence_detail/311.htm' target='right' class='red' >用户的个人信息，社交网络，地理位置，用户生成的数据和用户操作日志已经成倍的增加。</a><a href='../sentence_detail/312.htm' target='right' class='red' >我们如果要对这些用户数据进行挖掘，那SQL数据库已经不适合这些应用了， NoSQL数据库的发展也却能很好的处理这些大的数据。</a>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">140</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>3.2.2 MongoDB数据库简介</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">141</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><a href='../sentence_detail/314.htm' target='right' class='orange' >MongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。</a><a href='../sentence_detail/315.htm' target='right' class='red' >在高负载的情况下，添加更多的节点，可以保证服务器性能。</a><a href='../sentence_detail/316.htm' target='right' class='orange' >MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案，它 将数据存储为一个文档，数据结构由键值(key=]value)对组成。</a><span class='green'>MongoDB 文档类似于 JSON 对象。</span><span class='green'>字段值可以包含其他文档，数组及文档数组。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">142</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>1.</span><span class='green'>MongoDB主要特点</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">143</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><a href='../sentence_detail/321.htm' target='right' class='red' >MongoDB的提供面向文档存储，操作起来比较简单和容易。</a><a href='../sentence_detail/322.htm' target='right' class='orange' >我们可以在MongoDB记录中设置任何属性的索引来实现更快的排序。</a><a href='../sentence_detail/323.htm' target='right' class='red' >我们以通过本地或者网络创建数据镜像，这使得MongoDB有更强的扩展性。</a>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">144</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><a href='../sentence_detail/324.htm' target='right' class='red' >如果负载的增加（需要更多的存储空间和更强的处理能力），它可以分布在计算机网络中的其他节点上这就是所谓的分片。</a><span class='green'>MongoDB支持丰富的查询表达式。</span><a href='../sentence_detail/326.htm' target='right' class='red' >查询指令使用JSON形式的标记，可轻易查询文档中内嵌的对象及数组。</a><span class='green'>等等。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">145</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>2.</span><span class='green'>MongoDB的具体使用</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">146</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><a href='../sentence_detail/330.htm' target='right' class='orange' >我们将在下一章具体实现中详细介绍。</a>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">147</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>第4章 基于Scrapy框架的爬虫具体实现</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">148</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>第三章中对Scrapy框架以及MongoDB进行了大致的介绍，包括框架整体架构，以及如何创建一个Scrapy项目以及解释运行。</span><span class='green'>同时还指出，NoSQL数据库在爬虫项目中具有无可代替的灵活性，使用MongoDB作为数据存储的解决方案，契合主题而且十分正确。</span><span class='green'>另外在第二章中，我们还提到了在爬虫技术中将会遇到的不可避免的几个问题，如需要登录权限的网站，或者是Robot.txt对爬虫的限制。</span><span class='green'>在这章中，我们将会对以上提到的方面，做出更加详细可行的解决方案，结合Scrapy框架、MongoDB以及其他开源技术实现可定制的拓展性强的网络爬虫。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">149</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>4.1 爬虫总体设计介绍</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">150</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>4.1.1 爬取对象简介</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">151</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><a href='../sentence_detail/338.htm' target='right' class='orange' >本文实现的爬虫将以“新浪微博”为爬取目标，爬取网站用户的基本信息、用户的粉丝和其关注用户，</a><span class='green'>还将爬取用户所发表的微博内容，包括文本、定位、发送方式、以及获得的赞数和被转发数。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">152</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><a href='../sentence_detail/340.htm' target='right' class='orange' >“新浪微博”是国内著名的社交网站，有新浪网推出，提供类tweet博客服务。</a><span class='green'>选取“新浪微博”作为爬取对象，我们可以接触到很多方面的内容，如网页结构元素解析、登陆验证机制以及好友关系网的实现等诸多细节，</span><span class='green'>这将帮助我们更加细致的了解爬虫的工作原理和流程。</span><span class='green'>同时爬取得到的用户信息也极具价值，我们可以初步接触数据分析挖掘等知识，学习在大量数据规模下的趋势。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">153</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>4.1.2 总体架构设计</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">154</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><a href='../sentence_detail/345.htm' target='right' class='orange' >爬虫一共分为三个模块即前置规则预设模块、网页抓取模块、后续数据处理模块。</a>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">155</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>前置规则预设模块负责为程序提前设定了将要获取的数据的格式，更换User-agent以及更换Cookies登陆多个测试账号。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">156</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>网页抓取模块负责从定义的初始URL开始抓取网页信息，并进行初步提取分析，根据返回结果的类型调用不同的回调函数。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">157</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>后续数据处理模块则进一步分析网页抓取模块的结果，判定是否为合法的爬取结果，并将其存入数据库或者将此URL调至Scheduler等待调度爬取。</span><a href='../sentence_detail/349.htm' target='right' class='orange' >下面将对各个部分的实现细节进行介绍。</a>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">158</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>4.2 爬虫实现细节</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">159</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>4.2.1 前置规则预设模块</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">160</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>首先是对爬虫爬取数据格式的规定。</span><span class='green'>在item.py文件中，我们根据所要提取的字段设置了相应的类型。</span><span class='green'>如下图4-1所示。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">161</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>图4-1 item.py文件中的字段定义</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">162</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>class InformationItem:</span><span class='green'>定义了爬取用户信息的格式。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">163</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>class TweetsItem：</span><span class='green'>这个类定义的是发表微博相关内容格式。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">164</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>class FollowItem/class FanItem：</span><span class='green'>定义了爬取关注人和粉丝数据内容。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">165</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>下图4-2为InformationItem类的定义。</span><span class='green'>我们通过Field()方法来声明数据字段。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">166</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>图4-2 InformationItem类</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">167</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>其他类中的字段定义与InformationItem类似。</span><span class='green'>不再赘述。</span><span class='green'>涉及到有关Cookie设置和登录模块等问题将在下节中更加详细介绍。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">168</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>4.2.2 网页抓取模块</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">169</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>网页抓取模块的实现代码主要是在Spiders.py中，这是整个项目的核心部分。</span><span class='green'>在Spiders中我们定义了SinaSpider类。</span><span class='green'>如下图4-3所示。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">170</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>接下来我们将详细讲解Spider类的作用。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">171</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>图4-3 Spider类</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">172</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>1.</span><span class='green'>name和host属性:</span><span class='green'>定义了这个类的名字，是我们爬虫具有的唯一标识。</span><span class='green'>host属性定义了爬虫的爬取域，将在以“”该域名为主机的所有网页内进行爬取。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">173</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>2.</span><span class='green'>start_urls属性：</span><a href='../sentence_detail/380.htm' target='right' class='orange' >是一个初始URL列表，提供了爬虫开始运行时的目标，爬虫将从这些页面出发抓取数据。</a><span class='green'>这里我们采取了使用用户ID代替了较长的完整URL。</span><span class='green'>在接下来的start_requesets()方法中再将用户ID与host地址结合形成完整的初始URL。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">174</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>3.</span><span class='green'>Scrawl_ID以及finish_ID 为Set类型的变量。</span><span class='green'>Python中的set类型和其他语言相似，是一种无序不重复元素，基本功能包括消除重复元素等。</span><span class='green'>在这里我们使用Set类型的变量来标记已经爬取过的页面和等待调度爬取的页面，具有良好效果。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">175</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>4.请求处理部分</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">176</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>在SinaSpider中我们定义了以下的函数。</span><span class='green'>其中包括初始化Request并启动下载的start_requests()函数和四个回调函数。</span><span class='green'>回调函数分别处理抓取用户信息、微博内容、粉丝以及关注对象。</span><span class='green'>接下来将详细讲解start_requests()函数以及parse0()函数</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">177</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>图4-4 start_requests()函数</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">178</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>1）start_requests()负责读取在start_urls列表中的URL，生成了对应的Request对象，该对象将被作为参数被相应的回调函数调用。</span><span class='green'>关键代码如下：</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">179</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>ID = self.scrawl_ID.pop()</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">180</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>self.finish_ID.add(ID)</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">181</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>ID = str(ID)</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">182</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>url_follows = ”http:</span><span class='green'>//weibo.cn/%s/follow” % ID</span>
</p>
</div>


<div>
<span style="margin-left:25px"></span>
[
<a class="pagelink" href="paper_1.htm">首页</a>
<a class="pagelink" href="paper_3.htm">上一页</a>
<a class="pagelink" href="paper_5.htm">下一页</a>
<a class="pagelink" href="paper_6.htm">尾页</a>
页码：4/6页
]
</div>

<br>
<div style="margin-left:8px">

<div style="text-align:center;background-color:#CA122C;margin-top:30px;overflow:hidden;">
<a href="http://www.paperpass.com/publish/index?from=ppreport_banner" target="_blank" style="display:block;"><img height="180" src="http://file.paperpass.com/images/fabiao.jpg"></a>
</div>

</div>
</div>


<div class="zhengwencenter">
<p>
检测报告由<a href="http://www.paperpass.com/" target="_blank">PaperPass</a>文献相似度检测系统生成
</p>
<p>
Copyright © 2007-2016 PaperPass
</p>
</div>
<div style="margin-bottom:400px"></div>
</body>
</html>
