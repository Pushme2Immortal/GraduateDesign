<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN""http://www.w3.org/TR/html4/loose.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<title>PaperPass 最权威论文抄袭检测系统</title>
<style type="text/css">
<!--
user_icon {
color: #FFFFFF;
}
html
{
overflow-x:hidden;
overflow-y:auto;
}
body,td,th {
font-family: "微软雅黑";
font-size: 12px;
}
h1,h2,h3,h4,h5,h6 {
font-family: "宋体";
}
p{
margin-bottom:10px;
}
demo_padding {
line-height: 30px;
}
.zhengwen {
padding-right: 15px;
padding-left: 5px;
padding-bottom:100px;
font-size: 13px;
line-height: 20px;
color: #666666;
}
.zhengwencenter {
padding-right: 15px;
padding-left: 0px;
margin-bottom:10px;
font-size: 13px;
line-height: 20px;
color: #666666;
text-align:center
}
.neikuang {
background-color: #EBEBEB;
border: 1px solid #999999;
padding-right: 10px;
padding-left: 10px;
margin-top:10px;
margin-left:25px;
width:300px;
}
.shubu{
height: 20px;
width: 20px;
margin-left:25px;
background-color: #FFFFFF;
border: 1px solid #999999;
text-align: center;
vertical-align: middle;
display: block;
color: #666666;
}
a.red:link {color:#FF0000}
a.red:visited {color:#FF0000}
a.red:hover {color:#000000}
a.red:active {color:#000000}

a.orange:link {color:#FF6600}
a.orange:visited {color:#FF6600}
a.orange:hover {color:#000000}
a.orange:active {color:#000000}

a.dark:link {color:#666666}
a.dark:visited {color:#666666}
a.dark:hover {color:#000000}
a.dark:active {color:#000000}

a.pagelink:hover {color:#000000}
a.pagelink:active {color:#000000}

.green{color:#008000}
.gray{color:#666666}
.red{color:#FF0000}
.orange{color:#FF6600}
a{TEXT-DECORATION:none}

-->
</style>
</head>
<body>


<div class="zhengwen">
<div>
<span style="margin-left:25px"></span>
[
<a class="pagelink" href="paper_1.htm">首页</a>
<a class="pagelink" href="paper_4.htm">上一页</a>
<a class="pagelink" href="paper_6.htm">下一页</a>
<a class="pagelink" href="paper_6.htm">尾页</a>
页码：5/6页
]
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">183</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>取一个等待被调用的urllib，将其从待爬取队列中删去，并把这个id标记为已被爬取，加入finish_id。</span><span class='green'>再将这个ID与固定的URL前缀组成完整的初始URL，这里四个URL分别是代表了关注用户页面、粉丝页面、发表微博页面以及个人信息。</span><span class='green'>我们的爬虫将从这里开始抓取。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">184</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>yieldRequest(url=url_information0，meta={”ID”:</span><span class='green'>ID}， callback=self.parse0)</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">185</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>这句语句产生了一个请求，即yield a request，结合外层的while循环，展现了scrapy的追踪链接的机制：</span><span class='green'>当我们在回调函数中yield一个request之后，Scrapy将会调度，发送该请iu，并且在该请求完成时，调用所注册的回调函数。</span><span class='green'>callback = self.parse0 这个参数即代表该请求使用parse0作为回调函数，用于最终产生我们想要的数据。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">186</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>另外的同类型语句同理，不再赘述。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">187</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>图4-5 pasrse0()函数</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">188</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>2) parse0()函数负责抓取一部分的个人信息。</span><span class='green'>通过callback来处理yield的Request对象。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">189</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>selector = Selector(response)</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">190</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>text0=selector.xpath(’body/div[@class=”u”]/div[@class=”tip2”]’).extract_first(）</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">191</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>以上语句定义了选择器(Selector)，该选择器使用xpath方法，根据所要提取的微博用户信息文本在网页中的结构作为参数，解析网页并提取这一部分。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">192</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>yieldRequest(url=url_information1，meta={”item”:</span><span class='green'>informationItems}， callback=self.parse1)</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">193</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>parse0将分析返回的Request内容，返回 Item 对象、dict、 Request 或者一个包括三者的可迭代容器。</span><span class='green'>返回的Request对象之后会经过Scrapy处理，下载相应的内容，并调用设置的callback函数，与之前的start_requests()类似。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">194</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>其他的回调函数类似，不再赘述。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">195</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>4.2.3 后置数据处理模块</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">196</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>通过Spider类我们已经爬取到目标数据之后，我们需要考虑到数据的存储问题。</span><span class='green'>前一章讲到，对于爬虫爬取到的数据具有复杂结构，我们应该使用MongoDB来存储结果。</span><span class='green'>如何进行爬取结果和数据库之间的传送问题，是后置数据处理模块将要解决的问题。</span><span class='green'>主要代码集中在pipelines.py文件中。</span><span class='green'>关键代码如下：</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">197</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>class MongoDBPipleline(object):</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">198</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>def __init__(self):</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">199</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>clinet = pymongo.MongoClient(”localhost”， 27017)</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">200</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>db = clinet[”Sina”]</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">201</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>self.Information = db[”Information”]</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">202</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>self.Tweets = db[”Tweets”]</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">203</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>self.Follows = db[”Follows”]</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">204</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>self.Fans = db[”Fans”]</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">205</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>def process_item(self， item， spider):</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">206</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>if isinstance(item， InformationItem):</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">207</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>try:</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">208</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>self.Information.insert(dict(item))</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">209</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>except Exception:</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">210</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>pass</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">211</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>pipelines.py中定义了MondoDBPipeline类，包含了两个方法，分别用来初始化数据表和处理item并入库。</span><span class='green'>与mongodb的交互是通过pymongo开源模块实现的。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">212</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>clinet = pymongo.MongoClient(”localhost”， 27017)</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">213</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>打开了与数据库的本地连接，然后创建了名为”Sina“的数据库，接下来的语句分别创建Information、Tweet、Follows以及Fans四个表，用以存储对应的信息。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">214</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>process_item()函数负责判断item的类型，如判断出为InformationItem，则将其插入Information表中。</span><span class='green'>至此完成了后续数据处理。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">215</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>4.3 关键问题处理</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">216</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>在第二章中我们提到在设计爬虫中会遇到的一些关键问题，如突破目标网页对爬虫的限制、解决URL重复问题、多线程并发实现。</span><span class='green'>在这一节，我们将介绍在本研究中如何解决这些问题。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">217</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>4.3.1 网页登录与访问限制</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">218</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><a href='../sentence_detail/450.htm' target='right' class='orange' >新浪微博采取账号密码准入机制。</a><span class='green'>如果需要访问大量用户和微博内容，我们需要在程序中解决登陆问题和Robot.txt文件造成的访问限制。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">219</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>1.登陆问题的解决:</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">220</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>如上一章中对cookies的介绍，Cookies的合理利用可以使得爬虫使用cookies中保存的账号信息直接与浏览器和服务器进行交互，完成登陆。</span><span class='green'>但是新浪微博对于爬虫的限制比较严格，当使用单个账号进行快速爬取时，服务器很快将有反应来限制爬虫，即弹出302错误，让无法再获取资源。</span><span class='green'>所以我们将通过在cookies文件中写入大量微博账号，每隔一段时间自动更换，使得爬虫得以继续运行。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">221</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><a href='../sentence_detail/456.htm' target='right' class='orange' >而对于 Robot. txt的访问限制，我们分析得出， Robot. txt对爬虫的拒绝机制主要在于 User- agent的识别，</a><span class='green'>当以正常浏览器访问站点时不会出现问题，所以我们通过 user- agent池的方法将爬虫伪装成普通浏览器即可解决。</span><span class='green'>自动更换cookies和user-agent的代码如下：</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">222</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>class UserAgentMiddleware(object):</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">223</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>””” 换User-Agent ”””</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">224</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>def process_request(self， request， spider):</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">225</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>agent = random.choice(agents)</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">226</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>request.headers[”User-Agent”] = agent</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">227</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>class CookiesMiddleware(object):</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">228</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>””” 换Cookie ”””</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">229</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>def process_request(self， request， spider):</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">230</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>cookie = random.choice(cookies)</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">231</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>request.cookies = cookie</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">232</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>2.</span><span class='green'>URL去重：</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">233</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>Scrapy框架对URL去重问题有自带的便利的处理方法。</span><span class='green'>它是通过RFPDupe Filter 这个类实现的，具体来说是通过这个类里面的一个叫做 request_fingerprint 的方法来实现。</span><span class='green'>其中的关键代码如下所示:</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">234</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>def request_fingerprint(request， include_headers=None):</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">235</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>if include_headers:</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">236</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>include_headers = tuple([h.lower() for h in sorted(include_headers)])</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">237</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>cache = _fingerprint_cache.setdefault(request， {})</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">238</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>if include_headers not in cache:</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">239</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>fp = hashlib.sha1()</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">240</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>fp.update(request.method)</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">241</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>fp.update(canonicalize_url(request.url))</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">242</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>fp.update(request.body or ’’)</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">243</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>if include_headers:</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">244</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>for hdr in include_headers:</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">245</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>if hdr in request.headers:</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">246</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>fp.update(hdr)</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">247</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>cache[include_headers] = fp.hexd</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">248</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>由上述代码我们可以看到，去重用的信息指纹是利用四个部分通过 SHA1 算法计算得到，即 sha1(method + url + body + head)。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">249</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>3.多线程并发</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">250</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>并发是指同时处理的request的数量。</span><span class='green'>其有全局限制和局部(每个网站)的限制。</span><span class='green'>Scrapy默认的全局并发限制对同时爬取大量网站的情况并不适用，因此我们需要增加这个值。</span><span class='green'>增加多少取决于爬虫能占用多少CPU。</span><span class='green'>一般开始可以设置为 100 。</span>
</p>
</div>

<div>
<p>
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr>
<td align="left" width="50"><div class="shubu">251</div></td>
<td>&nbsp;&nbsp;</td>
</tr>
</table>

<span style="margin-left:25px"></span><span class='green'>根据我们的爬取目标，在spiders我们一共创建了四个Request请求，他们之间默认会采用多线程并发处理。</span><span class='green'>我们可以在settings.py中设置并发效率。</span><span class='green'>如图4-6所示：</span>
</p>
</div>


<div>
<span style="margin-left:25px"></span>
[
<a class="pagelink" href="paper_1.htm">首页</a>
<a class="pagelink" href="paper_4.htm">上一页</a>
<a class="pagelink" href="paper_6.htm">下一页</a>
<a class="pagelink" href="paper_6.htm">尾页</a>
页码：5/6页
]
</div>

<br>
<div style="margin-left:8px">

<div style="text-align:center;background-color:#CA122C;margin-top:30px;overflow:hidden;">
<a href="http://www.paperpass.com/publish/index?from=ppreport_banner" target="_blank" style="display:block;"><img height="180" src="http://file.paperpass.com/images/fabiao.jpg"></a>
</div>

</div>
</div>


<div class="zhengwencenter">
<p>
检测报告由<a href="http://www.paperpass.com/" target="_blank">PaperPass</a>文献相似度检测系统生成
</p>
<p>
Copyright © 2007-2016 PaperPass
</p>
</div>
<div style="margin-bottom:400px"></div>
</body>
</html>
